{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29e95c03-c84f-4e16-8b24-e49ff0ccce86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in /Users/danielfurman/Desktop/Berkeley/Semester-4/Capstone/.venv_capstone/lib/python3.8/site-packages (0.13.2)\n",
      "Collecting tokenizers\n",
      "  Using cached tokenizers-0.13.3-cp38-cp38-macosx_10_11_x86_64.whl (4.0 MB)\n",
      "Installing collected packages: tokenizers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.2\n",
      "    Uninstalling tokenizers-0.13.2:\n",
      "      Successfully uninstalled tokenizers-0.13.2\n",
      "Successfully installed tokenizers-0.13.3\n"
     ]
    }
   ],
   "source": [
    "#!python -m pip install --upgrade pip\n",
    "#!pip -q install --upgrade transformers\n",
    "#!pip -q install --upgrade accelerate\n",
    "#!pip -q install --upgrade torch\n",
    "#!pip -q install --upgrade sentencepiece\n",
    "#!pip install --upgrade protobuf==3.19.0\n",
    "!pip install --upgrade tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70caaf67-79b6-4ae7-8db9-70cf6ecebf10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.8\n"
     ]
    }
   ],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "080c6f44-2197-49bc-8f8c-0700f3b55ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import shutil\n",
    "#shutil.rmtree('llama/hf')\n",
    "#!mkdir llama\n",
    "#!mkdir 'llama/hf-7b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: apt-get\n",
      "\u001b[33mWARNING: Skipping crcmod as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting crcmod\n",
      "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: crcmod\n",
      "  Building wheel for crcmod (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for crcmod: filename=crcmod-1.7-cp38-cp38-macosx_10_9_x86_64.whl size=22358 sha256=a3824f5e3a4bb5c039d2c5f4101fbfe1854e3cc52ea73a129b097a3dea32bd57\n",
      "  Stored in directory: /private/var/folders/1t/v0bz2cw178lgvqvt64jr0fgm0000gn/T/pip-ephem-wheel-cache-sl9von9x/wheels/ca/5a/02/f3acf982a026f3319fb3e798a8dca2d48fafee7761788562e9\n",
      "Successfully built crcmod\n",
      "Installing collected packages: crcmod\n",
      "Successfully installed crcmod-1.7\n"
     ]
    }
   ],
   "source": [
    "!apt-get install gcc python3-dev python3-setuptools\n",
    "!pip3 uninstall crcmod\n",
    "!pip3 install --no-cache-dir -U crcmod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68b91edb-a4c7-4bcf-a379-2aff92edaf05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you experience problems with multiprocessing on MacOS, they might be related to https://bugs.python.org/issue33725. You can disable multiprocessing by editing your .boto config or by adding the following flag to your command: `-o \"GSUtil:parallel_process_count=1\"`. Note that multithreading is still available even if you disable multiprocessing.\n",
      "\n",
      "Copying gs://calibragpt/llama/7B/checklist.chk...\n",
      "Copying gs://calibragpt/llama/7B/consolidated.00.pth...\n",
      "Copying gs://calibragpt/llama/7B/params.json...\n",
      "\n",
      "Operation completed over 3 objects/12.6 GiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil -m cp -r gs://calibragpt/llama/7B llama/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2908a37-0da8-491d-b791-dfeeb8f2eb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://calibragpt/llama/llama.sh...\n",
      "\n",
      "Operation completed over 1 objects/1.9 KiB.                                      \n",
      "Copying gs://calibragpt/llama/tokenizer.model...\n",
      "\n",
      "Operation completed over 1 objects/488.0 KiB.                                    \n",
      "Copying gs://calibragpt/llama/tokenizer_checklist.chk...\n",
      "\n",
      "Operation completed over 1 objects/50.0 B.                                       \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp gs://calibragpt/llama/llama.sh llama\n",
    "!gsutil cp gs://calibragpt/llama/tokenizer.model llama\n",
    "!gsutil cp gs://calibragpt/llama/tokenizer_checklist.chk llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8504eb2e-cd95-4ffe-9ef0-b35ee7ca239e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m7B\u001b[m\u001b[m                      llama.sh                tokenizer_checklist.chk\n",
      "\u001b[34mhf-7b\u001b[m\u001b[m                   tokenizer.model\n"
     ]
    }
   ],
   "source": [
    "!ls llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca4f929e-b893-4ecf-88da-1377275d4497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'transformers'...\n",
      "remote: Enumerating objects: 144570, done.\u001b[K\n",
      "remote: Counting objects: 100% (814/814), done.\u001b[K\n",
      "remote: Compressing objects: 100% (496/496), done.\u001b[K\n",
      "remote: Total 144570 (delta 406), reused 590 (delta 266), pack-reused 143756\u001b[K\n",
      "Receiving objects: 100% (144570/144570), 147.92 MiB | 19.45 MiB/s, done.\n",
      "Resolving deltas: 100% (106944/106944), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "738d6e94-01ba-48ae-99cf-44c6aa10805f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA-weight-transfer.ipynb load_model.py\n",
      "\u001b[34mllama\u001b[m\u001b[m                       \u001b[34mtransformers\u001b[m\u001b[m\n",
      "load_model.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52f0649b-52e3-49c0-aff8-c181f458f456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching all parameters from the checkpoint at llama/7B.\n",
      "Loading the checkpoint in a Llama model.\n",
      "Loading checkpoint shards: 100%|████████████████| 33/33 [00:07<00:00,  4.47it/s]\n",
      "Saving in the Transformers format.\n",
      "Saving a LlamaTokenizerFast to llama/hf-7b.\n"
     ]
    }
   ],
   "source": [
    "!python transformers/src/transformers/models/llama/convert_llama_weights_to_hf.py \\\n",
    "    --input_dir llama --model_size 7B --output_dir llama/hf-7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfbf9442-0e43-4f80-bd37-3a90fc25489c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA-weight-transfer.ipynb load_model.py\n",
      "\u001b[34mllama\u001b[m\u001b[m                       \u001b[34mtransformers\u001b[m\u001b[m\n",
      "load_model.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70f2d565-977a-4db1-ad70-1a93e9f259d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielfurman/Desktop/Berkeley/Semester-4/Capstone/.venv_capstone/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:45<00:00, 52.84s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"llama/hf-7b\")\n",
    "model = LlamaForCausalLM.from_pretrained(\"llama/hf-7b\")\n",
    "model\n",
    "# tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8003e5f-e6ab-44d6-8085-4b832fbf0d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gsutil -m cp -r llama/hf gs://calibragpt/llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "444dbbf1-dcd9-4ef1-8e69-d49a767a1ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hey, are you conscious? Can you talk to me?\\nI'm not sure if you're conscious, but I'm going\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate\n",
    "generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53aa1d185f0c4d464253b7bca5e55e34e60de52cf1459f322cf3aa8af1e32b33"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('.venv_capstone': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
